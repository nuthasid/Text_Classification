{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tokenize\n",
    "## return tokenized list\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, n_gram, stop_en=None, stop_th=None, keyword=None):\n",
    "\n",
    "        import re\n",
    "        import deepcut\n",
    "        import os\n",
    "        from nltk.tokenize import TreebankWordTokenizer\n",
    "        from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "        self.test_text = 'ตัวอย่างความต้องการใช้ตัวอย่างความต้องการลีนุ๊กซ์การใช้ยากลำบาก'\n",
    "        self.pattern_thai_char = re.compile(u'[\\u0e00-\\u0e7f]')\n",
    "        self.pattern_new_sentence = re.compile('\\.[0-9]+(\\)|\\.) ')\n",
    "        self.pattern_th_out = re.compile(u'[\\u0e00-\\u0e7f][^\\u0e00-\\u0e7f]')\n",
    "        self.pattern_th_in = re.compile(u'[^\\u0e00-\\u0e7f][\\u0e00-\\u0e7f]')\n",
    "        self.pattern_num_bullet = re.compile('^[0-9]+(\\)|\\.)*$')\n",
    "        self.pattern_end_token = re.compile('^[a-zA-Z]+$')\n",
    "        self.pattern_number = re.compile('\\+*[0-9]+')\n",
    "        self.pattern_phone_number = re.compile('[0-9]+-[0-9]+-[0-9]+')\n",
    "        self.pattern_email = re.compile('[a-zA-Z._\\-0-9]+@[a-zA-Z._\\-0-9]+')\n",
    "        self.pattern_url = re.compile('(https://|www.)[a-zA-Z0-9]+.[a-z]+[^\\s]*')\n",
    "        self.pattern_sentence_collide = re.compile('[a-z][A-Z]]')\n",
    "        self.pattern_thai_name = re.compile(u'\\u0e04\\u0e38\\u0e13\\s*[\\u0e00-\\u0e7f]+\\s+')\n",
    "        self.charset = {}\n",
    "        with open(os.path.join(os.getcwd(), '..', 'dict', 'charset'), 'rt') as charfile:\n",
    "            for item in charfile.read().split('\\n'):\n",
    "                if len(item) < 4:\n",
    "                    self.charset[item] = ord(item)\n",
    "                else:\n",
    "                    self.charset[chr(int(item, 16))] = int(item, 16)\n",
    "        self.eng_tokenizer = TreebankWordTokenizer()\n",
    "        self.stemming = EnglishStemmer()\n",
    "        self.n_gram = n_gram\n",
    "        self.dp = deepcut\n",
    "        if stop_en:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', stop_en), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_en = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_en = set([])\n",
    "        if stop_th:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', stop_th), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_th = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_th = set([])\n",
    "        if keyword:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', keyword), 'rt', encoding='utf-8') as keyword_file:\n",
    "                self.keyword = set([item for item in keyword_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.keyword = set([])\n",
    "\n",
    "    def tokenizer(self, text=None):\n",
    "\n",
    "        def n_gram_compile(tokens, n):\n",
    "\n",
    "            tokens = tokens[:]\n",
    "            n_tokens = []\n",
    "            if n <= 1:\n",
    "                return tokens\n",
    "            for j, token in enumerate(tokens[:-(n - 1)]):\n",
    "                new_token = ''\n",
    "                for word in tokens[j:j + n]:\n",
    "                    if self.pattern_thai_char.search(word) and len(word) > 1:\n",
    "                        new_token += word\n",
    "                    else:\n",
    "                        new_token = ''\n",
    "                        break\n",
    "                if new_token:\n",
    "                    n_tokens.extend([new_token])\n",
    "            return n_tokens\n",
    "\n",
    "        def n_grams_compile(tokens, n):\n",
    "\n",
    "            if n < 2:\n",
    "                return tokens\n",
    "            n_tokens = []\n",
    "            for j in range(2, n + 1):\n",
    "                n_tokens.extend(n_gram_compile(tokens, j))\n",
    "            n_tokens = tokens + n_tokens\n",
    "            return n_tokens\n",
    "\n",
    "        def validate_char(val_text):\n",
    "            val_text = val_text.replace('&amp;', ' ')\n",
    "            val_text = val_text.replace('&nbsp;', ' ')\n",
    "            ret_text = ''\n",
    "            for cha in val_text:\n",
    "                try:\n",
    "                    self.charset[cha]\n",
    "                except KeyError:\n",
    "                    ret_text += ' '\n",
    "                else:\n",
    "                    ret_text += cha\n",
    "            while ret_text.find('  ') != -1:\n",
    "                ret_text = ret_text.replace('  ', ' ')\n",
    "            return ret_text\n",
    "\n",
    "        def split_th_en(splt_text):\n",
    "            insert_pos = []\n",
    "            splt_text = splt_text[:]\n",
    "            for pos, item in enumerate(splt_text[:-2]):\n",
    "                if self.pattern_th_in.search(splt_text[pos:pos+2]) or self.pattern_th_out.search(splt_text[pos:pos+2]):\n",
    "                    insert_pos.append(pos + 1)\n",
    "            for pos in reversed(insert_pos):\n",
    "                splt_text = splt_text[:pos] + ' ' + splt_text[pos:]\n",
    "            return splt_text\n",
    "\n",
    "        def remove_thai_stop(th_text):\n",
    "            stop_pos = [[0, 0]]\n",
    "            ## TH : do longest matching\n",
    "            for j in range(len(th_text)-1):\n",
    "                for k in range(j+1, len(th_text)):\n",
    "                    if th_text[j:k] in self.stop_th:\n",
    "                        # found keyword +++ instead of returning string - return positions that is\n",
    "                        # i to j\n",
    "                        if j <= stop_pos[-1][1]:\n",
    "                            stop_pos[-1] = [stop_pos[-1][0], k]\n",
    "                        else:\n",
    "                            stop_pos.append([j, k])\n",
    "                        break\n",
    "            newstr = ''\n",
    "            if len(stop_pos) == 1:\n",
    "                newstr = th_text\n",
    "            else:\n",
    "                for j in range(len(stop_pos)-1):\n",
    "                    newstr += th_text[stop_pos[j][1]:stop_pos[j+1][0]] + ' '\n",
    "            return newstr\n",
    "\n",
    "        if text == '-test':\n",
    "            text = self.test_text\n",
    "\n",
    "        text = text.replace(u'\\u0e46', ' ')\n",
    "        text = self.pattern_email.sub(' ', text)\n",
    "        text = self.pattern_url.sub(' ', text)\n",
    "        text = self.pattern_phone_number.sub(' ', text)\n",
    "        text = self.pattern_thai_name.sub(' ', text)\n",
    "        text = split_th_en(text)\n",
    "        text = self.pattern_new_sentence.sub(' . ', text)\n",
    "        text = text.replace('.', ' . ')\n",
    "        text = validate_char(text)\n",
    "        text = remove_thai_stop(text)\n",
    "        text_split = text.split(' ')\n",
    "        text_split = [item for item in text_split[:] if item not in self.stop_en\n",
    "                      and not self.pattern_num_bullet.search(item)]\n",
    "        text_split = [self.stemming.stem(item) if self.pattern_end_token.search(item) and\n",
    "                      item not in self.keyword else item for item in text_split[:]]\n",
    "\n",
    "        first_pass = []\n",
    "        for i, item in enumerate(text_split):\n",
    "            if self.pattern_sentence_collide.search(item) and item not in self.keyword:\n",
    "                c_text = self.pattern_sentence_collide.search(item)\n",
    "                first_pass.extend([c_text.string[:c_text.span()[0]+1], c_text.string[c_text.span()[1]-1:]])\n",
    "            else:\n",
    "                first_pass.append(item)\n",
    "        second_pass = []\n",
    "        for i, chunk in enumerate(first_pass):\n",
    "            if self.pattern_thai_char.search(chunk) and len(chunk) > 1:\n",
    "                new_chunk = self.dp.tokenize(chunk)\n",
    "                second_pass.extend(new_chunk)\n",
    "            else:\n",
    "                second_pass.append(chunk.lower())\n",
    "\n",
    "        second_pass = n_grams_compile(second_pass, self.n_gram)\n",
    "\n",
    "        return set(second_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                #if(count==100): break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "\n",
    "file_name = \"block1234.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data\n",
    "trainingData = data.getTrainingSet(\"0\")\n",
    "\n",
    "training_Desc = trainingData['desc'] \n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataMatrix[(data.dataMatrix['tag'] == \"0\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## create tokenizer\n",
    "tkn1 = Tokenizer(1, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "tkn2 = Tokenizer(2, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "tkn3 = Tokenizer(3, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "tkn4 = Tokenizer(4, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "\n",
    "## open vocab file\n",
    "#import os\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'desc_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    desc_vocab = f_tv.read().split('\\n')\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'title_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    title_vocab = f_tv.read().split('\\n')\n",
    "\n",
    "## create tfidf term-doc matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer)\n",
    "desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer)\n",
    "title_vec = title_vectorizer.fit_transform(training_Desc)\n",
    "\n",
    "## stack title onto desc\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([title_vec, desc_vec])\n",
    "\n",
    "## create label_vec\n",
    "label_vec = training_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77631149 0.77475156 0.76089342]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.79      0.82       106\n",
      "          1       0.79      0.85      0.82        99\n",
      "\n",
      "avg / total       0.82      0.82      0.82       205\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.85      0.89       341\n",
      "          1       0.86      0.95      0.90       341\n",
      "\n",
      "avg / total       0.90      0.90      0.90       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Train using Bernoulli NaiveBayes \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## see crossvalidation score\n",
    "bclf = BernoulliNB()\n",
    "scores = cross_val_score(bclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "\n",
    "## see train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "bclf = bclf.fit(desc_train, label_train)\n",
    "label_predict = bclf.predict(desc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test, label_predict))\n",
    "in_bclf = BernoulliNB()\n",
    "in_bclf = in_bclf.fit(data_vec, label_vec)\n",
    "label_predict = bclf.predict(data_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "def train_MNB(alpha):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "    ## see crossvalidation score\n",
    "    mclf = MultinomialNB(alpha=alpha)\n",
    "    scores = cross_val_score(mclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "    return sum(scores)/3\n",
    "\n",
    "    ## see train_test_split\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "    #mclf = mclf.fit(desc_train, label_train)\n",
    "    #label_predict = mclf.predict(desc_test)\n",
    "\n",
    "\n",
    "    #from sklearn.metrics import classification_report\n",
    "    #print(classification_report(label_test, label_predict))\n",
    "    #in_mclf = MultinomialNB(alpha=alpha)\n",
    "    #in_mclf = in_mclf.fit(data_vec, label_vec)\n",
    "    #label_predict = mclf.predict(data_vec)\n",
    "\n",
    "    #from sklearn.metrics import classification_report\n",
    "    #print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.0001934855588023865 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00016094804132793517 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00017669749049931127 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00016553578078248865 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00018166835026145467 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00022757640132797796 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.0002254350841328634 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00017460161035176558 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00019841076231419397 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/share/anaconda3/lib/python3.6/site-packages/scipy/optimize/zeros.py:195: RuntimeWarning: Tolerance of 0.00014266661276807202 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453096457733748 0.9349523308031198\n",
      "0.8438149905464515 0.6095608873005036\n",
      "0.8438149905464515 0.7670632537381816\n",
      "0.8438149905464515 0.6554405757158426\n",
      "0.8438149905464515 0.8167743367902573\n",
      "0.8408867404466127 1.275877801480601\n",
      "0.8408867404466127 1.2544635588703295\n",
      "0.8438149905464515 0.7461034043232557\n",
      "0.8438202539079657 0.9842068285229519\n",
      "0.8452233052756193 0.4267374609873982\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import newton\n",
    "import random\n",
    "roots = [newton(train_MNB, x0) for x0 in [random.random() + 0.3 for i in range(10)]]\n",
    "for item in roots:\n",
    "    print(train_MNB(item), item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8423775470079425 1.0\n",
      "0.8408867404466127 1.1\n",
      "0.8408867404466127 1.2\n",
      "0.8408867404466127 1.3\n",
      "0.8408867404466127 1.4\n",
      "0.8393920852196896 1.5\n",
      "0.840913932213803 1.6\n",
      "0.8394156698496268 1.7\n",
      "0.8394156698496268 1.8\n",
      "0.8394156698496268 1.9\n",
      "0.8394156698496268 2.0\n"
     ]
    }
   ],
   "source": [
    "alphas = [item / 10 for item in range(10,21,1)]\n",
    "for item in alphas:\n",
    "    print(train_MNB(item), item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682,)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.77111972e-01, 6.22888028e-01],\n",
       "       [2.12489120e-01, 7.87510880e-01],\n",
       "       [2.11919943e-01, 7.88080057e-01],\n",
       "       [5.33555688e-01, 4.66444312e-01],\n",
       "       [8.03297635e-02, 9.19670236e-01],\n",
       "       [3.64855723e-01, 6.35144277e-01],\n",
       "       [1.25997374e-01, 8.74002626e-01],\n",
       "       [7.88665460e-01, 2.11334540e-01],\n",
       "       [9.49491211e-01, 5.05087885e-02],\n",
       "       [1.98547732e-02, 9.80145227e-01],\n",
       "       [4.23958298e-03, 9.95760417e-01],\n",
       "       [5.62176866e-01, 4.37823134e-01],\n",
       "       [3.40298525e-01, 6.59701475e-01],\n",
       "       [2.54083625e-01, 7.45916375e-01],\n",
       "       [9.39635555e-01, 6.03644451e-02],\n",
       "       [7.05110852e-04, 9.99294889e-01],\n",
       "       [4.99019865e-01, 5.00980135e-01],\n",
       "       [1.89964852e-02, 9.81003515e-01],\n",
       "       [8.95617379e-01, 1.04382621e-01],\n",
       "       [7.16911508e-01, 2.83088492e-01],\n",
       "       [8.46527875e-01, 1.53472125e-01],\n",
       "       [7.42176098e-01, 2.57823902e-01],\n",
       "       [2.79384566e-01, 7.20615434e-01]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "mclf = mclf.fit(desc_train, label_train)\n",
    "label_predict_prob = mclf.predict_proba(desc_test)\n",
    "label_predict = mclf.predict(desc_test)\n",
    "label_predict_prob[label_predict != label_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(index=label_test[label_predict != label_test].index.values)\n",
    "frame['Label'] = list(label_test[label_predict != label_test])\n",
    "frame['prob_0'] = list(label_predict_prob[label_predict != label_test][:,0])\n",
    "frame['prob_1'] = list(label_predict_prob[label_predict != label_test][:,1])\n",
    "frame['title'] = [training_Title[item] for item in frame.index.values]\n",
    "frame['desc'] = [training_Desc[item] for item in frame.index.values]\n",
    "frame\n",
    "frame.to_csv('classification_error.csv', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(276, 323)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-f405a8b223d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_Title\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m276\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m323\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_Desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m276\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3103\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (276, 323)"
     ]
    }
   ],
   "source": [
    "print(training_Title[276,323])\n",
    "print(training_Desc[276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "def train_MNB(alpha):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "    ## see crossvalidation score\n",
    "    mclf = MultinomialNB(alpha=alpha)\n",
    "    scores = cross_val_score(mclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "\n",
    "    ## see train_test_split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "    mclf = mclf.fit(desc_train, label_train)\n",
    "    label_predict = mclf.predict(desc_test)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(label_test, label_predict))\n",
    "    in_mclf = MultinomialNB(alpha=alpha)\n",
    "    in_mclf = in_mclf.fit(data_vec, label_vec)\n",
    "    label_predict = mclf.predict(data_vec)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.79      0.86       112\n",
      "          1       0.79      0.94      0.86        93\n",
      "\n",
      "avg / total       0.87      0.86      0.86       205\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.87      0.91       341\n",
      "          1       0.88      0.97      0.92       341\n",
      "\n",
      "avg / total       0.92      0.92      0.92       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_MNB(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
