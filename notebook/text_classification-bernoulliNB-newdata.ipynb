{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tokenize\n",
    "## return tokenized list\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, n_gram, en_stop=None, th_stop=None):\n",
    "        import re\n",
    "        import deepcut\n",
    "        from nltk.tokenize import TreebankWordTokenizer\n",
    "        self.pattern = re.compile(u'[\\u0e01-\\u0e2e]')\n",
    "        self.eng_tokenizer = TreebankWordTokenizer()\n",
    "        self.n_gram = n_gram\n",
    "        self.dp = deepcut\n",
    "        if en_stop:\n",
    "            with open('\\\\dict\\\\' + en_stop, 'rt', encoding='utf-8') as stop_file:\n",
    "                self.en_stop = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.en_stop = set([])\n",
    "            \n",
    "    def tokenizer(self, text=None):\n",
    "        def n_gram_compile(tokens, n):\n",
    "            tokens = tokens[:]\n",
    "            n_tokens = []\n",
    "            if n <= 1:\n",
    "                return tokens\n",
    "            for j, token in enumerate(tokens[:-(n - 1)]):\n",
    "                new_token = ''\n",
    "                for word in tokens[j:j + n]:\n",
    "                    if self.pattern.search(word) and len(word) > 1:\n",
    "                        new_token += word\n",
    "                    else:\n",
    "                        new_token = ''\n",
    "                        break\n",
    "                if new_token:\n",
    "                    n_tokens.extend([new_token])\n",
    "            return n_tokens\n",
    "        \n",
    "        def n_grams_compile(tokens, n):\n",
    "            if n < 2:\n",
    "                return tokens\n",
    "            n_tokens = []\n",
    "            for j in range(2, n + 1):\n",
    "                n_tokens.extend(n_gram_compile(tokens, j))\n",
    "            n_tokens = tokens + n_tokens\n",
    "            return n_tokens\n",
    "        \n",
    "        in_text = text.replace('.', ' . ').replace(u'\\xa0', ' ').replace('  ', ' ')\n",
    "        first_pass = self.eng_tokenizer.tokenize(in_text)\n",
    "        first_pass = [item for item in first_pass[:] if item not in self.en_stop]\n",
    "        second_pass = []\n",
    "        for i, chunk in enumerate(first_pass):\n",
    "            if self.pattern.search(chunk) and len(chunk) > 1:\n",
    "                new_chunk = self.dp.tokenize(chunk)\n",
    "                second_pass.extend(new_chunk)\n",
    "            else:\n",
    "                second_pass.append(chunk.lower())\n",
    "        second_pass = n_grams_compile(second_pass, self.n_gram)\n",
    "        return second_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                #if(count==100): break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "\n",
    "file_name = \"data1_tonnytag.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data\n",
    "trainingData = data.getTrainingSet(\"1\")\n",
    "\n",
    "training_Desc = trainingData['desc'] \n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 3)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataMatrix[(data.dataMatrix['tag'] == \"1\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tokenizer\n",
    "tkn1 = Tokenizer(1)\n",
    "tkn2 = Tokenizer(2)\n",
    "tkn3 = Tokenizer(3)\n",
    "tkn4 = Tokenizer(4)\n",
    "\n",
    "## open vocab file\n",
    "import os\n",
    "with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'desc_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "    desc_vocab = f_tv.read().split('\\n')\n",
    "with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'title_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "    title_vocab = f_tv.read().split('\\n')\n",
    "\n",
    "## create tfidf term-doc matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer, vocabulary=desc_vocab)\n",
    "desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer, vocabulary=title_vocab)\n",
    "title_vec = title_vectorizer.fit_transform(training_Desc)\n",
    "\n",
    "## stack title onto desc\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([title_vec, desc_vec])\n",
    "\n",
    "## create label_vec\n",
    "label_vec = training_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66372425 0.71440228 0.65614035]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.04      0.08        49\n",
      "          1       0.47      1.00      0.64        41\n",
      "\n",
      "avg / total       0.76      0.48      0.33        90\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.05      0.09       149\n",
      "          1       0.51      1.00      0.68       149\n",
      "\n",
      "avg / total       0.76      0.52      0.38       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Train using Bernoulli NaiveBayes \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## see crossvalidation score\n",
    "bclf = BernoulliNB()\n",
    "scores = cross_val_score(bclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "\n",
    "## see train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "bclf = bclf.fit(desc_train, label_train)\n",
    "label_predict = bclf.predict(desc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test, label_predict))\n",
    "in_bclf = BernoulliNB()\n",
    "in_bclf = in_bclf.fit(data_vec, label_vec)\n",
    "label_predict = bclf.predict(data_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
