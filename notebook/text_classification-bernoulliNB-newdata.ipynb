{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tokenize\n",
    "## return tokenized list\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, n_gram, stop=None, keyword=None):\n",
    "\n",
    "        import re\n",
    "        import deepcut\n",
    "        import os\n",
    "        from nltk.tokenize import TreebankWordTokenizer\n",
    "        from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "        self.test_text = 'This is a test text. นี่เป็นตัวอย่าง ข้อความtesting ใน Python 3.6. ทดสอบtest.2) ทดสอบ3. test.4. '\n",
    "        self.thai_pattern = re.compile(u'[\\u0e00-\\u0e7f]')\n",
    "        self.new_sentence = re.compile('\\.[0-9]+(\\)|\\.) ')\n",
    "        self.pattern_th_out = re.compile(u'[\\u0e00-\\u0e7f][^\\u0e00-\\u0e7f]')\n",
    "        self.pattern_th_in = re.compile(u'[^\\u0e00-\\u0e7f][\\u0e00-\\u0e7f]')\n",
    "        self.num_bullet = re.compile('[0-9]+(\\)|\\.)*')\n",
    "        self.end_token = re.compile('^[a-zA-Z]+$')\n",
    "        self.charset = {}\n",
    "        with open(os.path.join(os.getcwd(), '..', 'dict', 'charset'), 'rt') as charfile:\n",
    "            for item in charfile.read().split('\\n'):\n",
    "                if len(item) < 4:\n",
    "                    self.charset[item] = ord(item)\n",
    "                else:\n",
    "                    self.charset[chr(int(item, 16))] = int(item, 16)\n",
    "        self.eng_tokenizer = TreebankWordTokenizer()\n",
    "        self.stemming = EnglishStemmer()\n",
    "        self.n_gram = n_gram\n",
    "        self.dp = deepcut\n",
    "        if stop:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', stop), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop = set([])\n",
    "        if keyword:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', keyword), 'rt', encoding='utf-8') as keyword_file:\n",
    "                self.keyword = set([item for item in keyword_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.keyword = set([])\n",
    "\n",
    "    def tokenizer(self, text=None):\n",
    "\n",
    "        def n_gram_compile(tokens, n):\n",
    "\n",
    "            tokens = tokens[:]\n",
    "            n_tokens = []\n",
    "            if n <= 1:\n",
    "                return tokens\n",
    "            for j, token in enumerate(tokens[:-(n - 1)]):\n",
    "                new_token = ''\n",
    "                for word in tokens[j:j + n]:\n",
    "                    if self.thai_pattern.search(word) and len(word) > 1:\n",
    "                        new_token += word\n",
    "                    else:\n",
    "                        new_token = ''\n",
    "                        break\n",
    "                if new_token:\n",
    "                    n_tokens.extend([new_token])\n",
    "            return n_tokens\n",
    "\n",
    "        def n_grams_compile(tokens, n):\n",
    "\n",
    "            if n < 2:\n",
    "                return tokens\n",
    "            n_tokens = []\n",
    "            for j in range(2, n + 1):\n",
    "                n_tokens.extend(n_gram_compile(tokens, j))\n",
    "            n_tokens = tokens + n_tokens\n",
    "            return n_tokens\n",
    "\n",
    "        def validate_char(val_text):\n",
    "            val_text = val_text.replace('&amp;', ' ')\n",
    "            ret_text = ''\n",
    "            for cha in val_text:\n",
    "                try:\n",
    "                    self.charset[cha]\n",
    "                except KeyError:\n",
    "                    ret_text += ' '\n",
    "                else:\n",
    "                    ret_text += cha\n",
    "            while ret_text.find('  ') != -1:\n",
    "                ret_text = ret_text.replace('  ', ' ')\n",
    "            return ret_text\n",
    "\n",
    "        def split_th_en(splt_text):\n",
    "            insert_pos = []\n",
    "            splt_text = splt_text[:]\n",
    "            for pos, item in enumerate(splt_text[:-2]):\n",
    "                if self.pattern_th_in.search(splt_text[pos:pos+2]) or self.pattern_th_out.search(splt_text[pos:pos+2]):\n",
    "                    insert_pos.append(pos + 1)\n",
    "            for pos in reversed(insert_pos):\n",
    "                splt_text = splt_text[:pos] + ' ' + splt_text[pos:]\n",
    "            return splt_text\n",
    "\n",
    "        if text == '-test':\n",
    "            text = self.test_text\n",
    "\n",
    "        text = split_th_en(text)\n",
    "        text = self.new_sentence.sub(' . ', text)\n",
    "        text = text.replace('. ', ' . ')\n",
    "        text = validate_char(text)\n",
    "        first_pass = text.split(' ')\n",
    "        first_pass = [item for item in first_pass[:] if item not in self.stop and not self.num_bullet.search(item)]\n",
    "        first_pass = [self.stemming.stem(item) if self.end_token.search(item) and\n",
    "                      item not in self.keyword else item for item in first_pass[:]]\n",
    "        second_pass = []\n",
    "        for i, chunk in enumerate(first_pass):\n",
    "            if self.thai_pattern.search(chunk) and len(chunk) > 1:\n",
    "                new_chunk = self.dp.tokenize(chunk)\n",
    "                second_pass.extend(new_chunk)\n",
    "            else:\n",
    "                second_pass.append(chunk.lower())\n",
    "\n",
    "        second_pass = n_grams_compile(second_pass, self.n_gram)\n",
    "\n",
    "        token_list = list(set(second_pass))\n",
    "\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                #if(count==100): break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "\n",
    "file_name = \"block123.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data\n",
    "trainingData = data.getTrainingSet(\"0\")\n",
    "\n",
    "training_Desc = trainingData['desc'] \n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dataMatrix[(data.dataMatrix['tag'] == \"1\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tokenizer\n",
    "tkn1 = Tokenizer(1)\n",
    "tkn2 = Tokenizer(2)\n",
    "tkn3 = Tokenizer(3)\n",
    "tkn4 = Tokenizer(4)\n",
    "\n",
    "## open vocab file\n",
    "#import os\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'desc_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    desc_vocab = f_tv.read().split('\\n')\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'title_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    title_vocab = f_tv.read().split('\\n')\n",
    "\n",
    "## create tfidf term-doc matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer)\n",
    "desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer)\n",
    "title_vec = title_vectorizer.fit_transform(training_Desc)\n",
    "\n",
    "## stack title onto desc\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([title_vec, desc_vec])\n",
    "\n",
    "## create label_vec\n",
    "label_vec = training_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71786376 0.73594691 0.71275697]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.59      0.66        80\n",
      "          1       0.68      0.82      0.74        84\n",
      "\n",
      "avg / total       0.72      0.71      0.70       164\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.72      0.81       272\n",
      "          1       0.77      0.94      0.85       272\n",
      "\n",
      "avg / total       0.85      0.83      0.83       544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Train using Bernoulli NaiveBayes \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## see crossvalidation score\n",
    "bclf = BernoulliNB()\n",
    "scores = cross_val_score(bclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "\n",
    "## see train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "bclf = bclf.fit(desc_train, label_train)\n",
    "label_predict = bclf.predict(desc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test, label_predict))\n",
    "in_bclf = BernoulliNB()\n",
    "in_bclf = in_bclf.fit(data_vec, label_vec)\n",
    "label_predict = bclf.predict(data_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
