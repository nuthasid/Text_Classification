{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean text\n",
    "file_name = 'data20180620.json'\n",
    "new_name = 'data20180620_cleaned.json'\n",
    "\n",
    "import sys\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "import os\n",
    "import json\n",
    "from clean_text import CleanText\n",
    "Clean_Text = CleanText('en_stop_word.txt', 'th_stop_word.txt')\n",
    "with open(os.path.join(os.getcwd(), '..', 'data', file_name), 'rt', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        job_ad = json.loads(line)\n",
    "        job_ad['title'] = Clean_Text.clean_text(job_ad['title'])\n",
    "        job_ad['desc'] = Clean_Text.clean_text(job_ad['desc'])\n",
    "        with open(os.path.join(os.getcwd(), '..', 'data', new_name), 'at', encoding='utf-8') as cleaned:\n",
    "            cleaned.write(json.dumps(job_ad, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        print('Begin loading')\n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                if count > 100: break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.dataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "\n",
    "file_name = \"masterDB_JPA Data - 20180406_flatten.json\"\n",
    "#file_name = 'data20180620.json'\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)\n",
    "## Create vectorized data\n",
    "data = data.getData()\n",
    "vec_Desc = data['desc'] \n",
    "vec_Title = data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tokenizer\n",
    "import sys\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "from tokenizer import Tokenizer\n",
    "import deepcut as dp\n",
    "def tokenizer(text):\n",
    "    return dp.tokenize(text)\n",
    "#tkn1 = Tokenizer(1, dp.deepcut)\n",
    "tkn2 = Tokenizer(2, tokenizer)\n",
    "#tkn3 = Tokenizer(3, dp.deepcut)\n",
    "tkn4 = Tokenizer(4, tokenizer)\n",
    "\n",
    "## open vocab file\n",
    "#import os\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'desc_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    desc_vocab = f_tv.read().split('\\n')\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'title_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    title_vocab = f_tv.read().split('\\n')\n",
    "\n",
    "## create tfidf term-doc matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer, min_df=0.85)\n",
    "desc_vectorizer.fit(vec_Desc)\n",
    "#desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer, min_df=0.85)\n",
    "title_vectorizer.fit(vec_Title)\n",
    "#title_vec = title_vectorizer.fit_transform(training_Desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train data\n",
    "file_name = \"data20180620.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "train_data = DataController(file_path)\n",
    "\n",
    "## Create training data\n",
    "trainingData = train_data.getTrainingSet(\"0\")\n",
    "\n",
    "training_Desc = trainingData['desc']\n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']\n",
    "\n",
    "desc_vec = desc_vectorizer.transform(training_Title)\n",
    "title_vec = title_vectorizer.transform(training_Desc)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([desc_vec, title_vec])\n",
    "label_vec = training_Label\n",
    "data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## see crossvalidation score\n",
    "cv = 3\n",
    "mclf = MultinomialNB(alpha=1)\n",
    "scores = cross_val_score(mclf, data_vec, label_vec, cv=cv, scoring='precision_macro')\n",
    "print('========== cv=' + str(cv) + ' cross-validation scores ==========')\n",
    "print(scores)\n",
    "print('================================================================')\n",
    "\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "mclf = mclf.fit(desc_train, label_train)\n",
    "label_predict = mclf.predict(data_vec)\n",
    "test_predict = mclf.predict(desc_test)\n",
    "\n",
    "print('===== Classification report from the whole TRAINING SET =====')\n",
    "print(classification_report(label_vec, label_predict))\n",
    "print('=============================================================')\n",
    "print('======= Classification report from the whole TEST set =======')\n",
    "print(classification_report(label_test, test_predict))\n",
    "print('=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(index=label_test[test_predict != label_test].index.values)\n",
    "label_predict_prob = mclf.predict_proba(desc_test)\n",
    "frame['Label'] = list(label_test[test_predict != label_test])\n",
    "frame['prob_0'] = list(label_predict_prob[test_predict != label_test][:,0])\n",
    "frame['prob_1'] = list(label_predict_prob[test_predict != label_test][:,1])\n",
    "frame['title'] = [training_Title[item] for item in frame.index.values]\n",
    "frame['desc'] = [training_Desc[item] for item in frame.index.values]\n",
    "frame.to_csv('classification_error.csv', encoding='utf-8', sep='\\t')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
