{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tokenize\n",
    "## return tokenized list\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, n_gram, stop_en=None, stop_th=None, keyword=None):\n",
    "\n",
    "        import re\n",
    "        import deepcut\n",
    "        import os\n",
    "        from nltk.tokenize import TreebankWordTokenizer\n",
    "        from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "        self.test_text = 'ตัวอย่างความต้องการใช้ตัวอย่างความต้องการลีนุ๊กซ์การใช้ยากลำบาก'\n",
    "        self.pattern_thai_char = re.compile(u'[\\u0e00-\\u0e7f]')\n",
    "        self.pattern_new_sentence = re.compile('\\.[0-9]+(\\)|\\.) ')\n",
    "        self.pattern_th_out = re.compile(u'[\\u0e00-\\u0e7f][^\\u0e00-\\u0e7f]')\n",
    "        self.pattern_th_in = re.compile(u'[^\\u0e00-\\u0e7f][\\u0e00-\\u0e7f]')\n",
    "        self.pattern_num_bullet = re.compile('^[0-9]+(\\)|\\.)*$')\n",
    "        self.pattern_end_token = re.compile('^[a-zA-Z]+$')\n",
    "        self.pattern_number = re.compile('\\+*[0-9]+')\n",
    "        self.pattern_phone_number = re.compile('[0-9]+-[0-9]+-[0-9]+')\n",
    "        self.pattern_email = re.compile('[a-zA-Z._\\-0-9]+@[a-zA-Z._\\-0-9]+')\n",
    "        self.pattern_url = re.compile('(https://|www.)[a-zA-Z0-9]+.[a-z]+[^\\s]*')\n",
    "        self.pattern_sentence_collide = re.compile('[a-z][A-Z]]')\n",
    "        self.pattern_thai_name = re.compile(u'\\u0e04\\u0e38\\u0e13\\s*[\\u0e00-\\u0e7f]+\\s+')\n",
    "        self.charset = {}\n",
    "        with open(os.path.join(os.getcwd(), '..', 'dict', 'charset'), 'rt') as charfile:\n",
    "            for item in charfile.read().split('\\n'):\n",
    "                if len(item) < 4:\n",
    "                    self.charset[item] = ord(item)\n",
    "                else:\n",
    "                    self.charset[chr(int(item, 16))] = int(item, 16)\n",
    "        self.eng_tokenizer = TreebankWordTokenizer()\n",
    "        self.stemming = EnglishStemmer()\n",
    "        self.n_gram = n_gram\n",
    "        self.dp = deepcut\n",
    "        if stop_en:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', stop_en), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_en = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_en = set([])\n",
    "        if stop_th:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', stop_th), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_th = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_th = set([])\n",
    "        if keyword:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'dict', keyword), 'rt', encoding='utf-8') as keyword_file:\n",
    "                self.keyword = set([item for item in keyword_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.keyword = set([])\n",
    "\n",
    "    def tokenizer(self, text=None):\n",
    "\n",
    "        def n_gram_compile(tokens, n):\n",
    "\n",
    "            tokens = tokens[:]\n",
    "            n_tokens = []\n",
    "            if n <= 1:\n",
    "                return tokens\n",
    "            for j, token in enumerate(tokens[:-(n - 1)]):\n",
    "                new_token = ''\n",
    "                for word in tokens[j:j + n]:\n",
    "                    if self.pattern_thai_char.search(word) and len(word) > 1:\n",
    "                        new_token += word\n",
    "                    else:\n",
    "                        new_token = ''\n",
    "                        break\n",
    "                if new_token:\n",
    "                    n_tokens.extend([new_token])\n",
    "            return n_tokens\n",
    "\n",
    "        def n_grams_compile(tokens, n):\n",
    "\n",
    "            if n < 2:\n",
    "                return tokens\n",
    "            n_tokens = []\n",
    "            for j in range(2, n + 1):\n",
    "                n_tokens.extend(n_gram_compile(tokens, j))\n",
    "            n_tokens = tokens + n_tokens\n",
    "            return n_tokens\n",
    "\n",
    "        def validate_char(val_text):\n",
    "            val_text = val_text.replace('&amp;', ' ')\n",
    "            val_text = val_text.replace('&nbsp;', ' ')\n",
    "            ret_text = ''\n",
    "            for cha in val_text:\n",
    "                try:\n",
    "                    self.charset[cha]\n",
    "                except KeyError:\n",
    "                    ret_text += ' '\n",
    "                else:\n",
    "                    ret_text += cha\n",
    "            while ret_text.find('  ') != -1:\n",
    "                ret_text = ret_text.replace('  ', ' ')\n",
    "            return ret_text\n",
    "\n",
    "        def split_th_en(splt_text):\n",
    "            insert_pos = []\n",
    "            splt_text = splt_text[:]\n",
    "            for pos, item in enumerate(splt_text[:-2]):\n",
    "                if self.pattern_th_in.search(splt_text[pos:pos+2]) or self.pattern_th_out.search(splt_text[pos:pos+2]):\n",
    "                    insert_pos.append(pos + 1)\n",
    "            for pos in reversed(insert_pos):\n",
    "                splt_text = splt_text[:pos] + ' ' + splt_text[pos:]\n",
    "            return splt_text\n",
    "\n",
    "        def remove_thai_stop(th_text):\n",
    "            stop_pos = [[0, 0]]\n",
    "            ## TH : do longest matching\n",
    "            for j in range(len(th_text)-1):\n",
    "                for k in range(j+1, len(th_text)):\n",
    "                    if th_text[j:k] in self.stop_th:\n",
    "                        # found keyword +++ instead of returning string - return positions that is\n",
    "                        # i to j\n",
    "                        if j <= stop_pos[-1][1]:\n",
    "                            stop_pos[-1] = [stop_pos[-1][0], k]\n",
    "                        else:\n",
    "                            stop_pos.append([j, k])\n",
    "                        break\n",
    "            newstr = ''\n",
    "            if len(stop_pos) == 1:\n",
    "                newstr = th_text\n",
    "            else:\n",
    "                for j in range(len(stop_pos)-1):\n",
    "                    newstr += th_text[stop_pos[j][1]:stop_pos[j+1][0]] + ' '\n",
    "            return newstr\n",
    "\n",
    "        if text == '-test':\n",
    "            text = self.test_text\n",
    "\n",
    "        text = text.replace(u'\\u0e46', ' ')\n",
    "        text = self.pattern_email.sub(' ', text)\n",
    "        text = self.pattern_url.sub(' ', text)\n",
    "        text = self.pattern_phone_number.sub(' ', text)\n",
    "        text = self.pattern_thai_name.sub(' ', text)\n",
    "        text = split_th_en(text)\n",
    "        text = self.pattern_new_sentence.sub(' . ', text)\n",
    "        text = text.replace('.', ' . ')\n",
    "        text = validate_char(text)\n",
    "        text = remove_thai_stop(text)\n",
    "        text_split = text.split(' ')\n",
    "        text_split = [item for item in text_split[:] if item not in self.stop_en\n",
    "                      and not self.pattern_num_bullet.search(item)]\n",
    "        text_split = [self.stemming.stem(item) if self.pattern_end_token.search(item) and\n",
    "                      item not in self.keyword else item for item in text_split[:]]\n",
    "\n",
    "        first_pass = []\n",
    "        for i, item in enumerate(text_split):\n",
    "            if self.pattern_sentence_collide.search(item) and item not in self.keyword:\n",
    "                c_text = self.pattern_sentence_collide.search(item)\n",
    "                first_pass.extend([c_text.string[:c_text.span()[0]+1], c_text.string[c_text.span()[1]-1:]])\n",
    "            else:\n",
    "                first_pass.append(item)\n",
    "        second_pass = []\n",
    "        for i, chunk in enumerate(first_pass):\n",
    "            if self.pattern_thai_char.search(chunk) and len(chunk) > 1:\n",
    "                new_chunk = self.dp.tokenize(chunk)\n",
    "                second_pass.extend(new_chunk)\n",
    "            else:\n",
    "                second_pass.append(chunk.lower())\n",
    "\n",
    "        second_pass = n_grams_compile(second_pass, self.n_gram)\n",
    "\n",
    "        return second_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                #if(count==100): break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "\n",
    "file_name = \"block1234.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data\n",
    "trainingData = data.getTrainingSet(\"0\")\n",
    "\n",
    "training_Desc = trainingData['desc'] \n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## create tokenizer\n",
    "# tkn1 = Tokenizer(1, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "# tkn2 = Tokenizer(2, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "# tkn3 = Tokenizer(3, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "# tkn4 = Tokenizer(4, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "tkn1 = Tokenizer(1)\n",
    "tkn2 = Tokenizer(2)\n",
    "tkn3 = Tokenizer(3)\n",
    "tkn4 = Tokenizer(4)\n",
    "\n",
    "## open vocab file\n",
    "#import os\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'desc_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    desc_vocab = f_tv.read().split('\\n')\n",
    "#with open(os.path.abspath(os.path.join(os.getcwd(), '..', 'dict', 'title_newdict_90p.txt'))  , 'rt', encoding='utf-8') as f_tv:\n",
    "#    title_vocab = f_tv.read().split('\\n')\n",
    "\n",
    "## create tfidf term-doc matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer)\n",
    "desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer)\n",
    "title_vec = title_vectorizer.fit_transform(training_Desc)\n",
    "\n",
    "## stack title onto desc\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([title_vec, desc_vec])\n",
    "\n",
    "## create label_vec\n",
    "label_vec = training_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76753939 0.74062434 0.65284178]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.46      0.58       108\n",
      "          1       0.59      0.85      0.69        97\n",
      "\n",
      "avg / total       0.68      0.64      0.63       205\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.65      0.76       341\n",
      "          1       0.73      0.96      0.83       341\n",
      "\n",
      "avg / total       0.83      0.80      0.80       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Train using Bernoulli NaiveBayes \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## see crossvalidation score\n",
    "bclf = BernoulliNB()\n",
    "scores = cross_val_score(bclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "\n",
    "## see train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "bclf = bclf.fit(desc_train, label_train)\n",
    "label_predict = bclf.predict(desc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test, label_predict))\n",
    "in_bclf = BernoulliNB()\n",
    "in_bclf = in_bclf.fit(data_vec, label_vec)\n",
    "label_predict = bclf.predict(data_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "def train_MNB(alpha):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "    ## see crossvalidation score\n",
    "    mclf = MultinomialNB(alpha=alpha)\n",
    "    scores = cross_val_score(mclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "    return sum(scores)/3\n",
    "\n",
    "    ## see train_test_split\n",
    "    #from sklearn.model_selection import train_test_split\n",
    "    #desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "    #mclf = mclf.fit(desc_train, label_train)\n",
    "    #label_predict = mclf.predict(desc_test)\n",
    "\n",
    "\n",
    "    #from sklearn.metrics import classification_report\n",
    "    #print(classification_report(label_test, label_predict))\n",
    "    #in_mclf = MultinomialNB(alpha=alpha)\n",
    "    #in_mclf = in_mclf.fit(data_vec, label_vec)\n",
    "    #label_predict = mclf.predict(data_vec)\n",
    "\n",
    "    #from sklearn.metrics import classification_report\n",
    "    #print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "def train_MNB(alpha):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "    ## see crossvalidation score\n",
    "    mclf = MultinomialNB(alpha=alpha)\n",
    "    scores = cross_val_score(mclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "\n",
    "    ## see train_test_split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "    mclf = mclf.fit(desc_train, label_train)\n",
    "    label_predict = mclf.predict(desc_test)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(label_test, label_predict))\n",
    "    in_mclf = MultinomialNB(alpha=alpha)\n",
    "    in_mclf = in_mclf.fit(data_vec, label_vec)\n",
    "    label_predict = mclf.predict(data_vec)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(label_vec, label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.74      0.76       104\n",
      "          1       0.75      0.78      0.76       101\n",
      "\n",
      "avg / total       0.76      0.76      0.76       205\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.80      0.85       341\n",
      "          1       0.82      0.92      0.87       341\n",
      "\n",
      "avg / total       0.86      0.86      0.86       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_MNB(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
